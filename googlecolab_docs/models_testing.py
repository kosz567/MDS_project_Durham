# -*- coding: utf-8 -*-
"""models_testing

Automatically generated by Colaboratory.

"""

from google.colab import files
uploaded_files = files.upload()

from google.colab import files
uploaded_files = files.upload()

from google.colab import files
uploaded_files = files.upload()

from google.colab import files
uploaded_files = files.upload()

from google.colab import files
uploaded_files = files.upload()

import csv
import pandas as pd
import nltk
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
from collections import defaultdict

#this is the same function that was used in previous code it is an established method for loading the MFD2.0 dictionary.
def load_dictionary(dic_file_path):
    dictionary = {}
    with open(dic_file_path, 'r') as file:
        for line in file:
            if not line.startswith('#'):
                line = line.strip()
                if line:
                    elements = line.split('\t')
                    if len(elements) != 2:
                        continue
                    word, category = elements
                    if category not in dictionary:
                        dictionary[category] = set()
                    dictionary[category].add(word)
    return dictionary

dictionary = load_dictionary('mfd2.0.dic')

print(dictionary)

## creating a merged dataframe which contains both the hand coded sample of data and all the remaining data, which will be used later for assigning labels to the full dataset.
df = pd.read_csv('sample_labelled1.csv')
df2 = pd.read_csv('remaining_data.csv')
merged_df1 = pd.concat([df, df2], ignore_index=True)

#this is the same code for merging the groups in the dictionary, so that it does not have the valence of the words.
dictionary['1'] = dictionary.get('1', set()) | dictionary.get('2', set())
dictionary['2'] = dictionary.get('3', set()) | dictionary.get('4', set())
dictionary['3'] = dictionary.get('5', set()) | dictionary.get('6', set())
dictionary['4'] = dictionary.get('7', set()) | dictionary.get('8', set())
dictionary['5'] = dictionary.get('9', set()) | dictionary.get('10', set())
#this code again deletes the empty categories which have been merged into another category.
del dictionary['6']
del dictionary['7']
del dictionary['8']
del dictionary['9']
del dictionary['10']

#Here the dictionary is updated again and saved.
import pickle

with open('updated_dictionary.dic', 'wb') as file:
    pickle.dump(dictionary, file)

"""This next section of code, is essentially doing the same as in the previous document however, this time i want to see the performance of the original MFD2.0, to see if hackenburg's (2021) customisation process actually improved the predictive performance of the dictionary for my dataset."""

from collections import Counter
import random
import re

#Again these are the same functions as previously used to assing values to columns in the dataframe for the customised dictionary, this process is repeated for the original.

column_to_analyze = 'text4'

def classify_row(row_text, dictionary):
    words = re.findall(r'\b\w+\b', row_text.lower())
    odd_categories = [category for category in dictionary.keys() if category[-1].isdigit() and int(category[-1]) % 2 != 0]
    even_categories = [category for category in dictionary.keys() if category[-1].isdigit() and int(category[-1]) % 2 == 0]

    odd_counts = Counter(word for word in words if any(word in dictionary[category] for category in odd_categories))
    even_counts = Counter(word for word in words if any(word in dictionary[category] for category in even_categories))

    if odd_counts and even_counts:
        max_odd_count = max(odd_counts.values())
        max_even_count = max(even_counts.values())
        if max_odd_count > max_even_count:
            return 1
        elif max_even_count > max_odd_count:
            return 2
        else:
            return random.choice([1, 2])
    elif odd_counts:
        return 1
    elif even_counts:
        return 2

    return 0

# Here the function is applied to the whole dataframe.
df['moral_sentiment1'] = df[column_to_analyze].apply(lambda x: classify_row(x, dictionary))


def classify_row(row_text, dictionary):
    words = re.findall(r'\b\w+\b', row_text.lower())
    categories_found = []
    flagged_words = []
    for category, words_list in dictionary.items():
        words_found = [word for word in words_list if word in words]
        if words_found:
            categories_found.append(category)
            flagged_words.extend(words_found)
    if categories_found:
        category_counts = {category: flagged_words.count(word) for category, word in zip(categories_found, flagged_words)}
        max_count = max(category_counts.values())
        max_categories = [category for category, count in category_counts.items() if count == max_count]
        chosen_category = random.choice(max_categories)
        return chosen_category, ', '.join(flagged_words)
    return "0", ""

# here the function is applied to the whole dataframe.
df['moral_mf_single1'], df['moral_words_flagged1'] = zip(*df[column_to_analyze].apply(lambda x: classify_row(x, dictionary)))

# This code makes sure that the labels that are dictionary aligned with the labels assigned during hand coding.
df['moral_mf_single1'] = df['moral_mf_single1'].astype(int)
df['recoded1'] = df['moral_mf_single1'].apply(lambda x: 6 if x in [2, 3, 5] else x)

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
#the same train test split as that used for the customised moral foundation dictionary.

X = df.drop(columns=['hand_coded1'])  # here i am getting the x data for the train test split
y_hand_codeds = df['hand_coded1']  # the first target variable for is the hand coded data
y_moral_sentiment = df['recoded1']  # the variable for it to be comapred to is the dictionary assigned values.

# here I am splitting the data into training and test sets for both so that the labels can be compared.
X_train, X_test, y_train_hand_codeds, y_test_hand_codeds, y_train_moral_sentiment, y_test_moral_sentiment = train_test_split(X, y_hand_codeds, y_moral_sentiment, test_size=0.2, random_state=42)

# This code computes the confusion matrix
conf_matrix_moral_sentiment = confusion_matrix(y_test_hand_codeds, y_test_moral_sentiment)

# this makes sure that the confusion matrix has the correct labels from the two columns
class_labels = sorted(y_hand_codeds.unique())
confusion_df_moral_sentiment = pd.DataFrame(conf_matrix_moral_sentiment,
                                             index=[f"True {label}" for label in class_labels],
                                             columns=[f"Predicted {label}" for label in class_labels])

# This next step visualises the confusion confusion matrix as a heatmap so that patters of misclassfication can be assessed.
plt.figure(figsize=(10, 8))
sns.heatmap(confusion_df_moral_sentiment, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix for hand_coded vs. recoded foundations")
plt.show()

from sklearn.metrics import classification_report

# now the classification report can be printed for the test set.
print("Classification Report for hand_codeds vs. (Test Set):")
print("Hand_codeds:")
print(classification_report(y_test_hand_codeds, y_test_moral_sentiment, digits=4))

X = df.drop(columns=['hand_codeds'])  # here i am getting the x data for the train test split (excluding the 'hand_codeds' column)
y_hand_codeds = df['hand_codeds']  # the first target variable for is the hand coded data
y_moral_sentiment = df['moral_sentiment1']  # the variable for it to be comapred to is the dictionary assigned values.

# here I am splitting the data into training and test sets for both 'hand_codeds' and 'moral_sentiment' so that the labelled can be compared.
X_train, X_test, y_train_hand_codeds, y_test_hand_codeds, y_train_moral_sentiment, y_test_moral_sentiment = train_test_split(X, y_hand_codeds, y_moral_sentiment, test_size=0.2, random_state=42)

# This code computes the confusion matrix for 'hand_coded1' and the 'moral_sentiment' columns.
conf_matrix_moral_sentiment = confusion_matrix(y_test_hand_codeds, y_test_moral_sentiment)

# this makes sure that the confusion matrix has the correct labels from the two columns
class_labels = sorted(y_hand_codeds.unique())
confusion_df_moral_sentiment = pd.DataFrame(conf_matrix_moral_sentiment,
                                             index=[f"True {label}" for label in class_labels],
                                             columns=[f"Predicted {label}" for label in class_labels])

# This next step visualises the confusion confusion matrix as a heatmap so that patters of misclassfication can be assessed.
plt.figure(figsize=(10, 8))
sns.heatmap(confusion_df_moral_sentiment, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix for hand_codeds vs. moral_sentiment")
plt.show()

from sklearn.metrics import classification_report

# now the classification report can be printed for the test set.
print("Classification Report for hand_codeds vs. moral sentiment (Test Set):")
print("Hand_codeds:")
print(classification_report(y_test_hand_codeds, y_test_moral_sentiment, digits=4))

"""WORD EMBEDDING MODEL CREATION, TRAINING AND TESTING  - this next section creates the word embedding model using the word2vec packages and word embeddings trained on the google news dataset.

The code for this model is adapted from here: https://colab.research.google.com/github/Hamxea/Multi-label-Classification/blob/master/_With_Word2Vec_Pre_trained_Embedding.ipynb#scrollTo=8tUlj_-X1Cz1, https://machinelearningmastery.com/develop-word-embeddings-python-gensim/, https://medium.com/@navmcgill/k-fold-cross-validation-in-keras-convolutional-neural-networks-835bed559d04, https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/, https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/

And also from these tutorials: https://www.youtube.com/watch?v=P47raNuzAW0
https://www.youtube.com/watch?v=EfEW3_RLnGA, https://www.youtube.com/watch?v=kKDYtZfriI8
"""

import gensim.downloader #here i am viewing all the datasets of pre-trained word-embeddings that are part of genism.

print(list(gensim.downloader.info()['models'].keys()))

# here I am downloading the "word2vec-google-news-300" model
model_name = "word2vec-google-news-300"
word2vec_model = gensim.downloader.load(model_name) # I sometimes had to run this twice as  the first time can stop dowloading especially if the runtime on colab disconnects as it takes a long time to download. be prepared to run it again if it does not work the first time.

#here I am importing all the required packages for using keras, so that I have use a word embedding based feedforward neural network classifier.
from sklearn.preprocessing import LabelEncoder
from gensim.models import KeyedVectors
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import Adam
from keras.utils import to_categorical
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split, StratifiedKFold
import nltk
import numpy as np
nltk.download('punkt')
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

# first load the labelled sample data
df = pd.read_csv('sample_labelled1.csv')

# In order to get the data in a suitable format the texts has to be tokenised into sentences so that it can be inputted into the model.
tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in df['text4']]

# size 300 embedding dimension chosen as this is correct for the "word2vec-google-news-300" dataset.
embedding_dim = 300

# here the tokens are converted into embeddings
embedding_matrix = np.zeros((len(tokenized_corpus), embedding_dim))

#this for loop iterates over every sentence in the tokenised corpus and creates the embedding matrix.
for i, sentence in enumerate(tokenized_corpus):
    word_embeddings = [word2vec_model[word] for word in sentence if word in word2vec_model]
    if word_embeddings:
        embedding_matrix[i] = np.mean(word_embeddings, axis=0)

# the hand coded labels also need to checked that they are all in the integer format.
df['hand_codeds'] = df['hand_codeds'].astype(int)

# the data is now split into train and testing data.
X = embedding_matrix
y = df['hand_codeds']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#as this is a multi-class classification problem the labels are one-hot encoded.
y_encoded = to_categorical(y_train, num_classes=3)
y_test_encoded = to_categorical(y_test, num_classes=3)

# this code sets up the k-fold cross validation with 5 folds.
k_folds = 5
skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)

# these are the different values of the hyperparameters which will be interated over.
epochs_list = [10, 25, 50]
batch_sizes = [16, 32, 64]

# this section creates a store for the results for each of different hyperparameter pairings.
results = []
best_val_accuracy = 0.0
best_epoch = 0

#iterating over all the batch sizes and all the epochs
for epochs in epochs_list:
    for batch_size in batch_sizes:
      #recording the validation accuracy.
        val_accuracies = []
#this code is for the k-fold cross cvalisation which splits the training data and test on a different fold each time.
        for train_idx, val_idx in skf.split(X_train, y_train):
            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]
            y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
            y_fold_train = to_categorical(y_fold_train, num_classes=3)
            y_fold_val = to_categorical(y_fold_val, num_classes=3)

            # This code sets the feed forwaard neural network model.
            model = Sequential()
            model.add(Dense(256, activation='relu', input_shape=(embedding_dim,)))
            model.add(Dropout(0.5))
            model.add(Dense(128, activation='relu'))
            model.add(Dropout(0.5))
            model.add(Dense(3, activation='softmax'))
            model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

            # this code trains the model for each fold in the cross validations
            history = model.fit(X_fold_train, y_fold_train, epochs=epochs, batch_size=batch_size, validation_data=(X_fold_val, y_fold_val), verbose=0)

            # the accuracy for each fold is the computed.
            val_loss, val_accuracy = model.evaluate(X_fold_val, y_fold_val)
            val_accuracies.append(val_accuracy)

        # Cto get one accuracy metric for each of the hyperparameter combinations tha average across all 5 folds is taken.
        avg_val_accuracy = np.mean(val_accuracies)
        if avg_val_accuracy > best_val_accuracy:
            best_val_accuracy = avg_val_accuracy
            best_epoch = epochs

        # this appends the results for each of the combinations of hyperparameters.
        results.append({
            'epochs': epochs,
            'batch_size': batch_size,
            'avg_val_accuracy': avg_val_accuracy,
            'history': history
        })

# Here I am printing the average accuracy for each combination of hyperparameters
print("Average Accuracy for Each Combination of Hyperparameters:")
for result in results:
    print(f"Epochs: {result['epochs']}, Batch Size: {result['batch_size']}, Avg. Val. Accuracy: {result['avg_val_accuracy']:.4f}")

# Here i am extracting the hyperparameter combination which has the best results.
best_results = [result for result in results if result['epochs'] == best_epoch and result['avg_val_accuracy'] == best_val_accuracy]
best_epochs = best_results[0]['epochs']
best_batch_size = best_results[0]['batch_size']
print("\nBest Hyperparameters:")
print(f"Epochs: {best_epochs}, Batch Size: {best_batch_size}, Avg. Val. Accuracy: {best_val_accuracy:.4f}")

#this section uses the best hyparameteters a chosen above to train the model on the full training data and the evaluate it on the test set.

# Print results for the best combination of hyperparameters
for result in best_results:
#extracting the best results again
    print("Epochs:", result['epochs'])
    print("Batch Size:", result['batch_size'])
    print("Average Validation Accuracy:", result['avg_val_accuracy'])

    # setting up the model again but this time with the best hyperparameters
    final_model = Sequential()
    final_model.add(Dense(256, activation='relu', input_shape=(embedding_dim,)))
    final_model.add(Dropout(0.5))
    final_model.add(Dense(128, activation='relu'))
    final_model.add(Dropout(0.5))
    final_model.add(Dense(128, activation='relu'))
    final_model.add(Dropout(0.5))
    final_model.add(Dense(3, activation='softmax'))
    final_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

    # Here the model is being training on the entire training data
    final_history = final_model.fit(X_train, y_encoded, epochs=result['epochs'], batch_size=result['batch_size'],
                                validation_data=(X_test, to_categorical(y_test, num_classes=3)), verbose=0)

    # this code plots the validation acc and val loss to see that accuracy is increaseing over epochs.
    plt.figure(figsize=(10, 6))
    plt.plot(final_history.history['val_accuracy'], label=f'Final Val Accuracy (Epochs={result["epochs"]}, Batch Size={result["batch_size"]})')
    plt.plot(final_history.history['val_loss'], label=f'Final Val Loss (Epochs={result["epochs"]}, Batch Size={result["batch_size"]})')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
    plt.xlabel('Epoch')
    plt.title(f'Final Validation Accuracy and Loss (Epochs={result["epochs"]}, Batch Size={result["batch_size"]})')
    plt.show()

   # Here i am evaluting the final model on the test set which has been set aside.
    loss, accuracy = final_model.evaluate(X_test, to_categorical(y_test, num_classes=3))
    y_pred = final_model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true_classes = y_test

    # This prints the full classification report for the final model
    class_report = classification_report(y_true_classes, y_pred_classes)
    print("Final Classification Report:")
    print(class_report)

import seaborn as sns
# This sets up the Confusion Matrix
confusion_mat = confusion_matrix(y_true_classes, y_pred_classes)

# Plot the Confusion Matrix as a Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

df_predictions =  pd.read_csv('BERT_PREDICTIONS.csv')
print(df_predictions.shape)

# in order to apply the model to the full text the texts in the text4 column first need to be preprocessed
tokenized_new_corpus = [word_tokenize(sentence.lower()) for sentence in df_predictions['text4']]

# this code creates the word embeddings
embedding_matrix_new = np.zeros((len(tokenized_new_corpus), embedding_dim))

for i, sentence in enumerate(tokenized_new_corpus):
     word_embeddings = [word2vec_model[word] for word in sentence if word in word2vec_model]
     if word_embeddings:
      embedding_matrix_new[i] = np.mean(word_embeddings, axis=0)

# this predictes the moral categories for the new data.
new_text_predictions = final_model.predict(embedding_matrix_new)
predicted_labels1 = np.argmax(new_text_predictions, axis=1)  # Get the index of the highest probability

# here the predictions are addred to a new column in the dataframe called moral_embeddings
df_predictions['moral_embeddings'] = predicted_labels1

"""WITHOUT K FOLD - MORAL SENTIMENT - WORD EMBEDDINGS - this section does exactly the same as the above but it is done without k-fold cross validation, to show the performance benefits of the this technique."""

# Preparing the data for training the neural network using the same embedding matric and the hand coded sentiment column.
X = embedding_matrix
y = df['hand_codeds']

# this time only a single train-validation split is created.
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# the labels are one hot encoded as normal.
y_encoded = to_categorical(y_train, num_classes=3)
y_val_encoded = to_categorical(y_val, num_classes=3)

# the same set of hyperparameters are searched over.
epochs_list = [10, 25, 50]
batch_sizes = [16, 32, 64]

# and this creates the store for the results for each set of hyperparamters.
results = []
best_val_accuracy = 0.0
best_epoch = 0


#this function is basically the same as the above but with the section for cross-validation removed.
for epochs in epochs_list:
    for batch_size in batch_sizes:
        model = Sequential()
        model.add(Dense(256, activation='relu', input_shape=(embedding_dim,)))
        model.add(Dropout(0.5))
        model.add(Dense(128, activation='relu'))
        model.add(Dropout(0.5))
        model.add(Dense(3, activation='softmax'))
        model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

        history = model.fit(X_train, y_encoded, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val_encoded), verbose=0)
        val_loss, val_accuracy = model.evaluate(X_val, y_val_encoded)
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            best_epoch = epochs
        results.append({
            'epochs': epochs,
            'batch_size': batch_size,
            'val_accuracy': val_accuracy,
            'history': history
        })

# the best resutls are then stored
best_results = [result for result in results if result['epochs'] == best_epoch and result['val_accuracy'] == best_val_accuracy]
best_epochs = best_results[0]['epochs']
best_batch_size = best_results[0]['batch_size']
print("\nBest Hyperparameters:")
print(f"Epochs: {best_epochs}, Batch Size: {best_batch_size}, Val. Accuracy: {best_val_accuracy:.4f}")

for result in best_results:
    print("Epochs:", result['epochs'])
    print("Batch Size:", result['batch_size'])

    final_model = Sequential()
    final_model.add(Dense(256, activation='relu', input_shape=(embedding_dim,)))
    final_model.add(Dropout(0.5))
    final_model.add(Dense(128, activation='relu'))
    final_model.add(Dropout(0.5))
    final_model.add(Dense(128, activation='relu'))
    final_model.add(Dropout(0.5))
    final_model.add(Dense(3, activation='softmax'))
    final_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
    final_history = final_model.fit(X_train, y_encoded, epochs=result['epochs'], batch_size=result['batch_size'],
                                    validation_data=(X_val, to_categorical(y_val, num_classes=3)), verbose=0)

    # this evaluates the  model on the test set.
    loss, accuracy = final_model.evaluate(X_val, to_categorical(y_val, num_classes=3))
    y_pred = final_model.predict(X_val)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true_classes = y_val

    # setting up the classification report.
    class_report = classification_report(y_true_classes, y_pred_classes)
    print("Final Classification Report:")
    print(class_report)

"""The above training and testing proceedure is the repeated but this time for the moral foundations classifier using the word2vec model. - WITH K FOLD"""

# as the labels for moral foundations are not continous a label mapping is created.
label_mapping = {0: 0, 1: 1, 4: 2, 6: 3}
# checking that the hand coded foundation labels are integers
df['hand_coded1'] = df['hand_coded1'].astype(int)
# this applies the mapping of the labels
y_mapped = df['hand_coded1'].map(label_mapping)
#the same as moral sentiment the labels are converted into one-hot-encoding form.
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y_mapped)
num_classes = len(label_encoder.classes_)
y_encoded_onehot = to_categorical(y_encoded, num_classes=num_classes)

# setting up for the train and testing split of the data.
X = embedding_matrix
y = y_encoded_onehot
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# here the number of folds for k-fold cross validation is set up.
k_folds = 5
skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)

# the lists of hyperparameters to iteratie over are then created.
epochs_list = [10, 25, 50]
batch_sizes = [16, 32, 64]

# Here a store for the results of the different hyperparameters is generated.
results = []
best_val_accuracy = 0.0
best_epoch = 0

#the for loop iterates over all combinations of bathc sizes and epochs
for epochs in epochs_list:
    for batch_size in batch_sizes:
        val_accuracies = []
#this is the ocde for perform cross validation on the 5 folds training one most and always keeping a fold back for testing.
        for train_idx, val_idx in skf.split(X_train, y_train.argmax(axis=1)):
            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]
            y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]

            # This code here sets the feedforward neural network model.
            model = Sequential()
            model.add(Dense(256, activation='relu', input_shape=(embedding_dim,)))
            model.add(Dropout(0.5))
            model.add(Dense(128, activation='relu'))
            model.add(Dropout(0.5))
            model.add(Dense(num_classes, activation='softmax'))
            model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

            # here the model is trained on the trinaing folds of the data in k-fold cross validation.
            history = model.fit(X_fold_train, y_fold_train, epochs=epochs, batch_size=batch_size, validation_data=(X_fold_val, y_fold_val), verbose=0)

            # this evaluates the model on the validation set
            val_loss, val_accuracy = model.evaluate(X_fold_val, y_fold_val)
            val_accuracies.append(val_accuracy)

        # Cto create on metric for each hyperparamter combination that accuracy accross all five folds is averaged.
        avg_val_accuracy = np.mean(val_accuracies)
        if avg_val_accuracy > best_val_accuracy:
            best_val_accuracy = avg_val_accuracy
            best_epoch = epochs

        # the results of each hyperparameter combination are then stored.
        results.append({
            'epochs': epochs,
            'batch_size': batch_size,
            'avg_val_accuracy': avg_val_accuracy,
            'history': history
        })

# the best results based on the best epoch and best validation accuracy is recorded and printed here
best_results = [result for result in results if result['epochs'] == best_epoch and result['avg_val_accuracy'] == best_val_accuracy]
best_epochs = best_results[0]['epochs']
best_batch_size = best_results[0]['batch_size']
print("\nBest Hyperparameters:")
print(f"Epochs: {best_epochs}, Batch Size: {best_batch_size}, Avg. Val. Accuracy: {best_val_accuracy:.4f}")

# As previously the best results for the hyperparameter training are used to build the final model.
for result in best_results:
    final_model = Sequential()
    final_model.add(Dense(256, activation='relu', input_shape=(embedding_dim,)))
    final_model.add(Dropout(0.5))
    final_model.add(Dense(128, activation='relu'))
    final_model.add(Dropout(0.5))
    final_model.add(Dense(num_classes, activation='softmax'))
    final_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

    # here the model is being trained on the full training set.
    final_history = final_model.fit(X_train, y_train, epochs=result['epochs'], batch_size=result['batch_size'],
                                    validation_data=(X_test, y_test), verbose=0)

    # here I am making a plot of the  validation Accuracy and Loss for the final model
    plt.figure(figsize=(10, 6))
    plt.plot(final_history.history['val_accuracy'], label=f'Final Val Accuracy (Epochs={result["epochs"]}, Batch Size={result["batch_size"]})')
    plt.plot(final_history.history['val_loss'], label=f'Final Val Loss (Epochs={result["epochs"]}, Batch Size={result["batch_size"]})')
    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
    plt.xlabel('Epoch')
    plt.title(f'Final Validation Accuracy and Loss (Epochs={result["epochs"]}, Batch Size={result["batch_size"]})')
    plt.show()

    # This evaluates the model on the test set.
    loss, accuracy = final_model.evaluate(X_test, y_test)
    y_pred = final_model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true_classes = np.argmax(y_test, axis=1)

    # This creates and prints the classification report.
    class_report = classification_report(y_true_classes, y_pred_classes)
    print("Final Classification Report:")
    print(class_report)

  #setting up the confuson matrix before plotting it as a heatmap.
confusion_mat = confusion_matrix(y_true_classes, y_pred_classes)
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_mat, annot=True, fmt="d", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')

##applying the model to the full dataframe for analysis later.

df_predictions['moral_embeddings2'] = predicted_labels1
df_predictions['moral_embeddings2'] = df_predictions['moral_embeddings2'].astype(int)

# makign the label coding match those assigned in teh hand coding process
df_predictions['moral_embeddings2'] = df_predictions['moral_embeddings2'].replace({2: 4, 3: 6})

# saving these to csv files
output_csv_filename = 'df_predictions_with_amended_moral_embeddings.csv'
df_predictions.to_csv(output_csv_filename, index=False)
filename = 'df_with_moral_embeddings.csv'
df_predictions.to_csv(filename, index=False)

"""This code repeat the above but this time it is for the moral foundation categories - THIS IS WITHOUR K-FOLD CROSS VALIDATION."""

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# non-continous labels require a label mapping to be in a suitable format for the model.
label_mapping = {0: 0, 1: 1, 4: 2, 6: 3}
df['hand_coded1'] = df['hand_coded1'].astype(int)

# here the mapping is being applied to the whole column
y_mapped = df['hand_coded1'].map(label_mapping)

# now the labels can be converted into to a numeric format using LabelEncoder to one-hot encode the labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y_mapped)
num_classes = len(label_encoder.classes_)
y_encoded_onehot = to_categorical(y_encoded, num_classes=num_classes)

# This is setting up the train test split to prepare for training the neural network
X = embedding_matrix
y = y_encoded_onehot
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Here I have creating the hyperparameters to search over
epochs_list = [10, 25, 50]
batch_sizes = [16, 32, 64]

# This stores the results for different hyperparameters
results = []
best_val_accuracy = 0.0
best_epoch = 0

for epochs in epochs_list:
    for batch_size in batch_sizes:
        # creating the feeforward neural network.
        model = Sequential()
        model.add(Dense(256, activation='relu', input_shape=(embedding_dim,)))
        model.add(Dropout(0.5))
        model.add(Dense(128, activation='relu'))
        model.add(Dropout(0.5))
        model.add(Dense(num_classes, activation='softmax'))
        model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

        # Training the model with validation set
        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), verbose=0)

        # This evaluates the model on the validation set
        val_loss, val_accuracy = model.evaluate(X_val, y_val)
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            best_epoch = epochs

        # and then storing the results for each combination of hyperparameters
        results.append({
            'epochs': epochs,
            'batch_size': batch_size,
            'avg_val_accuracy': val_accuracy,
            'history': history
        })

# Here I am storing and printing the best results based on the best epoch and best validation accuracy
best_results = [result for result in results if result['epochs'] == best_epoch and result['avg_val_accuracy'] == best_val_accuracy]
print("\nBest Hyperparameters:")
print(f"Epochs: {best_epochs}, Batch Size: {best_batch_size}, Best Val. Accuracy: {best_val_accuracy:.4f}")

#using the hyperparameters to build the final model
final_model = Sequential()
final_model.add(Dense(256, activation='relu', input_shape=(embedding_dim,)))
final_model.add(Dropout(0.5))
final_model.add(Dense(128, activation='relu'))
final_model.add(Dropout(0.5))
final_model.add(Dense(num_classes, activation='softmax'))
final_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
final_history = final_model.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size,
                                validation_data=(X_val, y_val), verbose=0)

# Here the Validation Accuracy and Loss for the final model are plotted
plt.figure(figsize=(10, 6))
plt.plot(final_history.history['val_accuracy'], label=f'Final Val Accuracy (Epochs={best_epochs}, Batch Size={best_batch_size})')
plt.plot(final_history.history['val_loss'], label=f'Final Val Loss (Epochs={best_epochs}, Batch Size={best_batch_size})')
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
plt.xlabel('Epoch')
plt.title(f'Final Validation Accuracy and Loss (Epochs={best_epochs}, Batch Size={best_batch_size})')
plt.show()

# This then tests the model on the test set
loss, accuracy = final_model.evaluate(X_val, y_val)
y_pred = final_model.predict(X_val)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(y_val, axis=1)

#the classification report and confusion matrix can now be printed
class_report = classification_report(y_true_classes, y_pred_classes)
print("Final Classification Report:")
print(class_report)
confusion_mat = confusion_matrix(y_true_classes, y_pred_classes)
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_mat, annot=True, fmt="d", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')

"""The following section is for the DistilBERT model for both moral foundations and moral sentiment, this is first done with k-fold then with out. The code for the DistilBERT model is directly taken and adapted from Melanie Walsh's AI for Humanisits tutorials here: http://www.bertforhumanists.org/tutorials/ and https://colab.research.google.com/drive/19jDqa5D5XfxPU6NQef17BC07xQdRnaKU?usp=sharing.

There code for the cross-validation is adpated from the sources use for it when it was applied to the word embedding model.
"""

!pip install transformers[torch]
import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW
from transformers import TrainingArguments, Trainer
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
from transformers import DistilBertForSequenceClassification, TrainingArguments, Trainer
from sklearn.metrics import accuracy_score

import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.metrics import accuracy_score
import numpy as np
from sklearn.model_selection import KFold
from transformers import Trainer, TrainingArguments
from sklearn.model_selection import cross_val_score
df = pd.read_csv('sample_labelled1.csv')

#here the data is first split into training and testing datasets.
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# the texts and their associated labels are extracted as lists for the train and test datasets
train_texts = list(train_df['text4'])
train_labels = list(train_df['hand_codeds'])
test_texts = list(test_df['text4'])
test_labels = list(test_df['hand_codeds'])

#importing the dataset feature to so that a custom dataset can be compiled.
from torch.utils.data import Dataset

class MyDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# This creates and applies the tokeniser from the distil bert uncased pre-trained dataset.
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# the maximium length of the encodings is set here.
max_length = 128

# the tokenizer can now be applied to the train and test texts.
train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)
test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length)

# the labels are encoded
unique_labels = set(label for label in train_labels)
label2id = {label: id for id, label in enumerate(unique_labels)}
train_labels_encoded = [label2id[y] for y in train_labels]
test_labels_encoded = [label2id[y] for y in test_labels]

# the custom dataset function is applied to encodings and their associated labels.
train_dataset = MyDataset(train_encodings, train_labels_encoded)
test_dataset = MyDataset(test_encodings, test_labels_encoded)


#Here I am making use of the free access to a limited time of GPU access to speed up the running time for the model.
device_name = 'cuda' if torch.cuda.is_available() else 'cpu'

# This is setting up the model.
model_name = 'distilbert-base-uncased'
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id)).to(device_name)

# here I am creating the function which is used to traine the hyperparameters with k-fold cross-validation
def hyperparameter_tuning(batch_size, num_epochs):
    # setting the number of folds
    num_folds = 2
    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)

    accuracies = []
#cross validation code adapted from the used by the word2vec model.
    for train_index, val_index in kf.split(train_texts):
        train_texts_fold = [train_texts[i] for i in train_index]
        train_labels_fold = [train_labels_encoded[i] for i in train_index]

        val_texts_fold = [train_texts[i] for i in val_index]
        val_labels_fold = [train_labels_encoded[i] for i in val_index]

        # for each fold the data has to be
        train_encodings_fold = tokenizer(train_texts_fold, truncation=True, padding=True, max_length=max_length)
        val_encodings_fold = tokenizer(val_texts_fold, truncation=True, padding=True, max_length=max_length)

        # for each fold the custom dataset is created.
        train_dataset_fold = MyDataset(train_encodings_fold, train_labels_fold)
        val_dataset_fold = MyDataset(val_encodings_fold, val_labels_fold)

        # the standard training arguments are set up as used by the BERT tutorial.
        training_args = TrainingArguments(
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=20,
            learning_rate=5e-5,
            warmup_steps=100,
            weight_decay=0.01,
            output_dir='./results',
            logging_dir='./logs',
            logging_steps=100,
            evaluation_strategy='steps',
            disable_tqdm=False,
        )

        # Computing the metrics to assess the models predictions.
        def compute_metrics(pred):
            labels = pred.label_ids
            preds = pred.predictions.argmax(-1)
            acc = accuracy_score(labels, preds)
            return {'accuracy': acc}

        # This creates the trainer for the model.
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset_fold,
            eval_dataset=val_dataset_fold,
            compute_metrics=compute_metrics
        )

        # The model is the trained and evaluated on the validation folds.
        trainer.train()
        results = trainer.evaluate(val_dataset_fold)
        accuracies.append(results['eval_accuracy'])

    return np.mean(accuracies) # finally the function returns the average accuracy across all the folds for a particular combination of hyperparameters.

# instead of defining a full grid the hyperparameters have to be manually entered to save on the use of the GPU.
batch_sizes = [16]
num_epochs_list = [5]

#this creates a store for the accuracy of th model
best_accuracy = 0.0
best_batch_size = None
best_num_epochs = None

# This code applies the batch sizes and epochs given in the grid.
for batch_size in batch_sizes:
    for num_epochs in num_epochs_list:
        accuracy = hyperparameter_tuning(batch_size, num_epochs)
        print(f"Batch Size: {batch_size}, Epochs: {num_epochs}, Accuracy: {accuracy:.4f}")

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_batch_size = batch_size
            best_num_epochs = num_epochs

print(f"Best Batch Size: {best_batch_size}, Best Epochs: {best_num_epochs}, Best Accuracy: {best_accuracy:.4f}")

#After manually going through some hyperparameters the best a selected here.
best_batch_size = 16
best_num_epochs = 5


# Now the model can ber trained on the full training data
full_training_args = TrainingArguments(
    num_train_epochs=best_num_epochs,
    per_device_train_batch_size=best_batch_size,
    per_device_eval_batch_size=20,
    learning_rate=5e-5,
    warmup_steps=100,
    weight_decay=0.01,
    output_dir='./results_full_training',
    logging_dir='./logs_full_training',
    logging_steps=100,
    evaluation_strategy='steps',
    disable_tqdm=False,
)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    return {'test_accuracy': acc}

trainer_full_training = Trainer(
    model=model,
    args=full_training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

trainer_full_training.train()

# The model is now evaluated
test_results = trainer_full_training.evaluate()

print("Test Set Evaluation Results:", test_results)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# this cnverts the labels back to string before getting the predicted labels and comparing them to the true labels
class_names = [str(label) for label in unique_labels]
#this prints the classification report and confusion matrix for the BERT model.
pred_labels = trainer_full_training.predict(test_dataset).predictions.argmax(axis=1)
conf_matrix = confusion_matrix(test_labels_encoded, pred_labels)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix")
plt.show()
report = classification_report(test_labels_encoded, pred_labels, target_names=class_names)
print("Classification Report:\n", report)

import random
 #using the coding tutoral this extracts some of the misclassified texts and shows what label they were misclassified as.
pred_labels = trainer_full_training.predict(test_dataset).predictions.argmax(axis=1)

correct_predictions = 0
misclassified_examples = []

for _true_label, _predicted_label, _text in random.sample(list(zip(test_labels, pred_labels, test_texts)), 25):
    if _true_label == _predicted_label:
        correct_predictions += 1
    else:
        misclassified_examples.append((_true_label, _predicted_label, _text))

print("Number of correct predictions:", correct_predictions)
print("\nMisclassified examples:")
for _true_label, _predicted_label, _text in misclassified_examples:
    print('TRUE LABEL:', _true_label)
    print('PREDICTED LABEL:', _predicted_label)
    print('REVIEW TEXT:', _text)
    print()

## this code then applied the model to predicted the labels for all the texts in the data set.

# Firstly the data is tokenised
new_texts = list(merged_df1['text4'])
new_encodings = tokenizer(new_texts, truncation=True, padding=True, max_length=max_length)

# Second a custom dataset is created for the full merged data set.
new_dataset = MyDataset(new_encodings, [0] * len(new_texts))

# Here a new data loader is created
new_dataloader = torch.utils.data.DataLoader(new_dataset, batch_size=best_batch_size, shuffle=False)
model.eval()
predictions = []

with torch.no_grad():
    for batch in new_dataloader:
        inputs = {key: val.to(device_name) for key, val in batch.items()}
        outputs = model(**inputs)
        preds = outputs.logits.argmax(dim=-1)
        predictions.extend(preds.cpu().numpy())

# Lastly the predictions are extracted and added as new column in the dataset.
merged_df1['predicted_labels2'] = predictions

"""THIS CODE REPEATS THE ABOVE BUT WITHOUT THE K-FOLD CROSS VALIDATION."""

# As performed previously the code is split into the test and train dataframes
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# the labels for the test and training texts are extracted
train_texts = list(train_df['text4'])
train_labels = list(train_df['hand_codeds'])
test_texts = list(test_df['text4'])
test_labels = list(test_df['hand_codeds'])

#creating the same function for loading a custom dataset.
class MyDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# This creates the necessary tokeniser from the pretained datasets avaliable from BERT.
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# This is the max sequence length for the encodings.
max_length = 128

# These two lines of code tokenise the test and training data.
train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)
test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length)

# Here the labels are encoded.
unique_labels = set(label for label in train_labels)
label2id = {label: id for id, label in enumerate(unique_labels)}
train_labels_encoded = [label2id[y] for y in train_labels]
test_labels_encoded = [label2id[y] for y in test_labels]

# the dataset function is applied to create the custom datasets.
train_dataset = MyDataset(train_encodings, train_labels_encoded)
test_dataset = MyDataset(test_encodings, test_labels_encoded)
#making use of the limited additional gpu processing power.
device_name = 'cuda' if torch.cuda.is_available() else 'cpu'
#setting up the model.
model_name = 'distilbert-base-uncased'
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id)).to(device_name)

# This is all wrapping into one function for hyperparameter tuning without k-fold cross-validation
def hyperparameter_tuning(batch_size, num_epochs):
    # the encoded dataset is tokenised.
    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)
    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length)

    # the customised datasets are creates and the the same training arguments are defined as previously.
    train_dataset = MyDataset(train_encodings, train_labels_encoded)
    val_dataset = MyDataset(val_encodings, val_labels_encoded)
    training_args = TrainingArguments(
        num_train_epochs=num_epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=20,
        learning_rate=5e-5,
        warmup_steps=100,
        weight_decay=0.01,
        output_dir='./results',
        logging_dir='./logs',
        logging_steps=100,
        evaluation_strategy='steps',
        disable_tqdm=False,
    )

    # using the same comput metrics function as shown in the tutorial.
    def compute_metrics(pred):
        labels = pred.label_ids
        preds = pred.predictions.argmax(-1)
        acc = accuracy_score(labels, preds)
        return {'accuracy': acc}

    # The trainer is also created in the same way.
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics
    )

    # Here the model is trained and then evaluated using the validation set.
    trainer.train()

    # Evaluate the model
    results = trainer.evaluate(val_dataset)
    return results['eval_accuracy']

# Again a manual grid of hyperparameters has to be used due to the limited processing power.
batch_sizes = [32]
num_epochs_list = [5]

best_accuracy = 0.0
best_batch_size = None
best_num_epochs = None

# Here the single train, test and validation split is created
train_texts, val_texts, train_labels_encoded, val_labels_encoded = train_test_split(train_texts, train_labels_encoded, test_size=0.2, random_state=42)

# the batch size and epoch is applied here.
for batch_size in batch_sizes:
    for num_epochs in num_epochs_list:
        accuracy = hyperparameter_tuning(batch_size, num_epochs)
        print(f"Batch Size: {batch_size}, Epochs: {num_epochs}, Accuracy: {accuracy:.4f}")

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_batch_size = batch_size
            best_num_epochs = num_epochs
#this print the accuracy.
print(f"Best Batch Size: {best_batch_size}, Best Epochs: {best_num_epochs}, Best Accuracy: {best_accuracy:.4f}")

# here I am applying the best hyperparamters to train the model fully,
best_batch_size = 32
best_num_epochs = 5

#  This code trains the model on the full training data
full_training_args = TrainingArguments(
    num_train_epochs=best_num_epochs,
    per_device_train_batch_size=best_batch_size,
    per_device_eval_batch_size=20,
    learning_rate=5e-5,
    warmup_steps=100,
    weight_decay=0.01,
    output_dir='./results_full_training',
    logging_dir='./logs_full_training',
    logging_steps=100,
    evaluation_strategy='steps',
    disable_tqdm=False,
)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    return {'test_accuracy': acc}

trainer_full_training = Trainer(
    model=model,
    args=full_training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

trainer_full_training.train()
#the final model is then evaluated.
test_results = trainer_full_training.evaluate()

print("Test Set Evaluation Results:", test_results)

#this extracts the labels, gets the predicted labels and then prints the classification report.
class_names = [str(label) for label in unique_labels]
pred_labels = trainer_full_training.predict(test_dataset).predictions.argmax(axis=1)
report = classification_report(test_labels_encoded, pred_labels, target_names=class_names)
print("Classification Report:\n", report)

"""This section does the same as the above but for moral foundations, for the DistilBERT model."""

#same as above just using the hand-coded1 column instead.
train_texts = list(train_df['text4'])
train_labels = list(train_df['hand_coded1'])

test_texts = list(test_df['text4'])
test_labels = list(test_df['hand_coded1'])
#custom dataset function the same as the abvoe
class MyDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

#creates the tokeniser and also the length of the encoded sequence
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
max_length = 128
train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)
test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length)
unique_labels = set(label for label in train_labels)
label2id = {label: id for id, label in enumerate(unique_labels)}
#here the labels are encoded
train_labels_encoded = [label2id[y] for y in train_labels]
test_labels_encoded = [label2id[y] for y in test_labels]
#this applies the custom dataset function to the test and training datasets
train_dataset = MyDataset(train_encodings, train_labels_encoded)
test_dataset = MyDataset(test_encodings, test_labels_encoded)

# making use of the additional GPU access
device_name = 'cuda' if torch.cuda.is_available() else 'cpu'
#this creates the model
model_name = 'distilbert-base-uncased'
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id)).to(device_name)

#this is a function which is used to apply k-fold cross validation to the training of the hyperparamters,
def hyperparameter_tuning(batch_size, num_epochs):
    num_folds = 2
    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)
#again due to limited computing power a reduced number of folds has to be used.
    accuracies = []
#creating the training and validation sets for the number of fold given,
    for train_index, val_index in kf.split(train_texts):
        train_texts_fold = [train_texts[i] for i in train_index]
        train_labels_fold = [train_labels_encoded[i] for i in train_index]

        val_texts_fold = [train_texts[i] for i in val_index]
        val_labels_fold = [train_labels_encoded[i] for i in val_index]

        # for each fold the encodings have to be tokenised
        train_encodings_fold = tokenizer(train_texts_fold, truncation=True, padding=True, max_length=max_length)
        val_encodings_fold = tokenizer(val_texts_fold, truncation=True, padding=True, max_length=max_length)

        # the cusotmised dataset function is applied to the current train and validation set
        train_dataset_fold = MyDataset(train_encodings_fold, train_labels_fold)
        val_dataset_fold = MyDataset(val_encodings_fold, val_labels_fold)

        # As done previously the training arguments are defined.
        training_args = TrainingArguments(
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=20,
            learning_rate=5e-5,
            warmup_steps=100,
            weight_decay=0.01,
            output_dir='./results',
            logging_dir='./logs',
            logging_steps=100,
            evaluation_strategy='steps',
            disable_tqdm=False,
        )

        # Same function again
        def compute_metrics(pred):
            labels = pred.label_ids
            preds = pred.predictions.argmax(-1)
            acc = accuracy_score(labels, preds)
            return {'accuracy': acc}

        # Same creation of the training and then training the model.
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset_fold,
            eval_dataset=val_dataset_fold,
            compute_metrics=compute_metrics
        )

        trainer.train()

        # Lastly the model is evaluated
        results = trainer.evaluate(val_dataset_fold)
        accuracies.append(results['eval_accuracy'])

    return np.mean(accuracies) # the average accruacy acorss all fold for the particular hyperparameters is calculated and returned.

# manually setting the hyperparamters.
batch_sizes = [32]
num_epochs_list = [5]

best_accuracy = 0.0
best_batch_size = None
best_num_epochs = None
#same code as used previously to iterate over the defined hyperparameters
for batch_size in batch_sizes:
    for num_epochs in num_epochs_list:
        accuracy = hyperparameter_tuning(batch_size, num_epochs)
        print(f"Batch Size: {batch_size}, Epochs: {num_epochs}, Accuracy: {accuracy:.4f}")

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_batch_size = batch_size
            best_num_epochs = num_epochs

print(f"Best Batch Size: {best_batch_size}, Best Epochs: {best_num_epochs}, Best Accuracy: {best_accuracy:.4f}")

# Again this code applies the best hyperparamters to the full traing set.
best_batch_size = 16
best_num_epochs = 5

# Train the model on the full training data
full_training_args = TrainingArguments(
    num_train_epochs=best_num_epochs,
    per_device_train_batch_size=best_batch_size,
    per_device_eval_batch_size=20,
    learning_rate=5e-5,
    warmup_steps=100,
    weight_decay=0.01,
    output_dir='./results_full_training',
    logging_dir='./logs_full_training',
    logging_steps=100,
    evaluation_strategy='steps',
    disable_tqdm=False,
)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    return {'test_accuracy': acc}

trainer_full_training = Trainer(
    model=model,
    args=full_training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

trainer_full_training.train()
test_results = trainer_full_training.evaluate()

print("Test Set Evaluation Results:", test_results)

# Convert unique labels to strings
class_names = [str(label) for label in unique_labels]

pred_labels = trainer_full_training.predict(test_dataset).predictions.argmax(axis=1)

# Calculate and print the classification report
report = classification_report(test_labels_encoded, pred_labels, target_names=class_names)
print("Classification Report:\n", report)

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Calculate the confusion matrix
cm = confusion_matrix(test_labels_encoded, pred_labels)

# Plot the confusion matrix heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix Heatmap')
plt.show()

import random
#extracting the predicted labels from the full trained and evaluated datast
pred_labels = trainer_full_training.predict(test_dataset).predictions.argmax(axis=1)

# applying a label mapping
predicted_label_mapping = {0: 0, 1: 1, 2: 4, 3: 6}
mapped_pred_labels = [predicted_label_mapping[label] for label in pred_labels]

# Printing some correct and misclassified examples as show in the AI humanist tutorial
correct_predictions = 0
misclassified_examples = []

for _true_label, _predicted_label, _text in random.sample(list(zip(test_labels, mapped_pred_labels, test_texts)), 25):
    if _true_label == _predicted_label:
        correct_predictions += 1
    else:
        misclassified_examples.append((_true_label, _predicted_label, _text))

print("Number of correct predictions:", correct_predictions)
print("\nMisclassified examples:")
for _true_label, _predicted_label, _text in misclassified_examples:
    print('TRUE LABEL:', _true_label)
    print('PREDICTED LABEL:', _predicted_label)
    print('REVIEW TEXT:', _text)
    print()

#this code applies the BERT model to the full dataset, getting the label predictions for each and adding them into a new column of predicted labels.
new_texts = list(merged_df1['text4'])
new_encodings = tokenizer(new_texts, truncation=True, padding=True, max_length=max_length)

new_dataset = MyDataset(new_encodings, [0] * len(new_texts))
new_dataloader = torch.utils.data.DataLoader(new_dataset, batch_size=best_batch_size, shuffle=False)

#this code makes the predictions on the new data.
model.eval()
predictions = []

with torch.no_grad():
    for batch in new_dataloader:
        inputs = {key: val.to(device_name) for key, val in batch.items()}
        outputs = model(**inputs)
        preds = outputs.logits.argmax(dim=-1)
        predictions.extend(preds.cpu().numpy())

merged_df1['predicted_labels'] = predictions
#this makes sure that the predicted labels are correctly coded in line with those orginially done in the hand coding of the dataset.
label_mapping = {0: 0, 1: 1, 2: 4, 3: 6}
merged_df1['predicted_labels'] = merged_df1['predicted_labels'].replace(label_mapping)

#merged_df1.to_csv('BERT_PREDICTIONS.csv', index=False) #saving the dataset to a csv file. # uncomment to save the file.
